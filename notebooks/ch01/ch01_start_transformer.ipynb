{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch01_start_transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZnfAd3ZVqUeSsY4hJyTxe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psygrammer/psypy_lm/blob/main/notebooks/ch01/ch01_start_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXlLT63Zt6Ep"
      },
      "source": [
        "# 1. Getting Started with the Model Architecture of the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L74LPx4qpdXs"
      },
      "source": [
        "# Contents\n",
        "* The background of the Transformer\n",
        "* The rise of the Tranformer : Attention is All You Need\n",
        "  - The encoder stack\n",
        "    - Input embedding\n",
        "    - Positional encoding\n",
        "    - Sub-layer 1 : Multi-head attention\n",
        "    - Sub-layer 2 : Feedforward network\n",
        "  - The decoder stack\n",
        "    - Output embedding and position encoding\n",
        "    - The attention layers\n",
        "    - The FFN sub-layer, the Post-LN, and the linear layer\n",
        "* Training and performance\n",
        "  - Before we end the chapter\n",
        "* Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQcWmmEqqM62"
      },
      "source": [
        "# The background of the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz-jy_kAt37V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfPJonPNqh8b"
      },
      "source": [
        "# The rise of the Tranformer : Attention is All You Need\n",
        "* The encoder stack\n",
        "* The decoder stack\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6X_vrlwqqsx"
      },
      "source": [
        "## The encoder stack\n",
        "* Input embedding\n",
        "* Positional encoding\n",
        "* Sub-layer 1 : Multi-head attention\n",
        "* Sub-layer 2 : Feedforward network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfi8ReXEuK2n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MotCa3y2rKAB",
        "outputId": "329ce9c3-201a-498a-86f0-4fd622e6c9cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!pip install --upgrade gensim\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhQxFJhHtXHR"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsB8tbxVuGBf"
      },
      "source": [
        "# change work_dir\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1ouFHvzq2Dn"
      },
      "source": [
        "### Input embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GrjGWyHrKpR"
      },
      "source": [
        "input_str = \"the Transformer is an innovative NLP model!\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozxspHBotnh6",
        "outputId": "3d4a787f-8e2a-4cdc-be51-72420c11164b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens = word_tokenize(input_str)\n",
        "tokens"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'Transformer', 'is', 'an', 'innovative', 'NLP', 'model', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOx325pFt4wM"
      },
      "source": [
        "# word2vec을 사용하기 위해 학습을 해보자. \n",
        "# 아래처럼 코퍼스 파일이 있는 상황."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woBLoD94q7OX"
      },
      "source": [
        "### Positional encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZBmYXHorLLd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "634lIkTZq8kl"
      },
      "source": [
        "### Sub-layer 1 : Multi-head attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3aRhbHMrLub"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7WWnFmdq9ou"
      },
      "source": [
        "### Sub-layer 2 : Feedforward network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNtcfwK9rMLs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ74C6f8quu_"
      },
      "source": [
        "## The decoder stack\n",
        "* Output embedding and position encoding\n",
        "* The attention layers\n",
        "* The FFN sub-layer, the Post-LN, and the linear layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcS3r3eBrNPF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVeNyRlxq_1R"
      },
      "source": [
        "### Output embedding and position encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ-IL6QbrOts"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-dhpxwBrFaU"
      },
      "source": [
        "### The attention layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iVWzZztrPJ2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYbsmkTzrGag"
      },
      "source": [
        "### The FFN sub-layer, the Post-LN, and the linear layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgLWM04srPq5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icoqdONCqw1v"
      },
      "source": [
        "# Training and performance\n",
        "* Before we end the chapter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlmqxjD1qyo4"
      },
      "source": [
        "# Summary"
      ]
    }
  ]
}