{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch01_start_transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOz1on89vVFpYH64sQH5Diw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psygrammer/psypy_lm/blob/main/notebooks/ch01/ch01_start_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXlLT63Zt6Ep"
      },
      "source": [
        "# 1. Getting Started with the Model Architecture of the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L74LPx4qpdXs"
      },
      "source": [
        "# Contents\n",
        "* The background of the Transformer\n",
        "* The rise of the Tranformer : Attention is All You Need\n",
        "  - The encoder stack\n",
        "    - Input embedding\n",
        "    - Positional encoding\n",
        "    - Sub-layer 1 : Multi-head attention\n",
        "    - Sub-layer 2 : Feedforward network\n",
        "  - The decoder stack\n",
        "    - Output embedding and position encoding\n",
        "    - The attention layers\n",
        "    - The FFN sub-layer, the Post-LN, and the linear layer\n",
        "* Training and performance\n",
        "  - Before we end the chapter\n",
        "* Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQcWmmEqqM62"
      },
      "source": [
        "# The background of the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz-jy_kAt37V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfPJonPNqh8b"
      },
      "source": [
        "# The rise of the Tranformer : Attention is All You Need\n",
        "* The encoder stack\n",
        "* The decoder stack\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6X_vrlwqqsx"
      },
      "source": [
        "## The encoder stack\n",
        "* Input embedding\n",
        "* Positional encoding\n",
        "* Sub-layer 1 : Multi-head attention\n",
        "* Sub-layer 2 : Feedforward network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfi8ReXEuK2n",
        "outputId": "b97e1ca9-0d6d-4f1e-b018-06d4d92ed829"
      },
      "source": [
        "!git clone https://github.com/psygrammer/psypy_lm.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'psypy_lm'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 22 (delta 4), reused 9 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (22/22), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wOtJp6vvUtq",
        "outputId": "ec8c2545-0ddb-4b7b-fa7d-7c623f88609e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mpsypy_lm\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsB8tbxVuGBf",
        "outputId": "57591f9d-eb50-46af-b62d-47ad96ebd37d"
      },
      "source": [
        "# change work_dir\n",
        "%cd /content/psypy_lm/notebooks/ch01/"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/psypy_lm/notebooks/ch01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5xpMBqSvcCl",
        "outputId": "2c2a10a1-feeb-4061-9d05-ce813d492889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ch01_start_transformer.ipynb  text.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MotCa3y2rKAB",
        "outputId": "329ce9c3-201a-498a-86f0-4fd622e6c9cd"
      },
      "source": [
        "#!pip install --upgrade gensim\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhQxFJhHtXHR"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1ouFHvzq2Dn"
      },
      "source": [
        "### Input embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GrjGWyHrKpR"
      },
      "source": [
        "input_str = \"the Transformer is an innovative NLP model!\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozxspHBotnh6",
        "outputId": "3d4a787f-8e2a-4cdc-be51-72420c11164b"
      },
      "source": [
        "tokens = word_tokenize(input_str)\n",
        "tokens"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'Transformer', 'is', 'an', 'innovative', 'NLP', 'model', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOx325pFt4wM"
      },
      "source": [
        "# word2vec을 사용하기 위해 학습을 해보자. \n",
        "# 아래처럼 코퍼스 파일이 있는 상황."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woBLoD94q7OX"
      },
      "source": [
        "### Positional encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZBmYXHorLLd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "634lIkTZq8kl"
      },
      "source": [
        "### Sub-layer 1 : Multi-head attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3aRhbHMrLub"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7WWnFmdq9ou"
      },
      "source": [
        "### Sub-layer 2 : Feedforward network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNtcfwK9rMLs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ74C6f8quu_"
      },
      "source": [
        "## The decoder stack\n",
        "* Output embedding and position encoding\n",
        "* The attention layers\n",
        "* The FFN sub-layer, the Post-LN, and the linear layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcS3r3eBrNPF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVeNyRlxq_1R"
      },
      "source": [
        "### Output embedding and position encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ-IL6QbrOts"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-dhpxwBrFaU"
      },
      "source": [
        "### The attention layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iVWzZztrPJ2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYbsmkTzrGag"
      },
      "source": [
        "### The FFN sub-layer, the Post-LN, and the linear layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgLWM04srPq5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icoqdONCqw1v"
      },
      "source": [
        "# Training and performance\n",
        "* Before we end the chapter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlmqxjD1qyo4"
      },
      "source": [
        "# Summary"
      ]
    }
  ]
}